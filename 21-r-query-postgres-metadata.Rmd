# Getting metadata about and from the database (21)

```{r setup, echo=FALSE, message=FALSE, warning=FALSE}
# depending on your working directory, run one or the other of these two:
# source("r-database-docker/book-src/standard-package-list.R")
source("book-src/standard-package-list.R")
``` 
Note that `tidyverse`, `DBI`, `RPostgres`, `glue`, and `knitr` are loaded.  Also, we've sourced the [`db-login-batch-code.R`]('r-database-docker/book-src/db-login-batch-code.R') file which is used to log in to PostgreSQL.

```{r echo=FALSE}
read_chunk("book-src/db-login-batch-code.R")

# use when debugging outside of knitr
# source('r-database-docker/book-src/db-login-batch-code.R')
# source('book-src/db-login-batch-code.R')
# source('r-database-docker/book-src/db-login-interactive-code.R')
```
For this chapter R needs the `dbplyr` package to access `alternate schemas`.  A [schema](http://www.postgresqltutorial.com/postgresql-server-and-database-objects/) is an object that contains one or more tables.  Most often there will be a default schema, but to access the metadata, you need to explicitly specify which schema contains the data you want.

```{r}
library(dbplyr)
```

```{r get_postgres_connection, eval=TRUE, echo=FALSE}

```

Assume that the Docker container with PostgreSQL and the dvdrental database are ready to go. 
```{r}
system2("docker", "start sql-pet", stdout = TRUE, stderr = TRUE)
```
Connect to the database:
```{r}
con <- wait_for_postgres(
  user = Sys.getenv("DEFAULT_POSTGRES_USER_NAME"),
  password = Sys.getenv("DEFAULT_POSTGRES_PASSWORD"),
  dbname = "dvdrental",
  seconds_to_test = 10
)
```
## Always look at the data

### Browse a few rows of a table

So far in this books we've most often looked at the data by listing a few observations or using a tool like `glimpse`.
```{r}
rental <- dplyr::tbl(con, "rental")

kable(head(rental))
glimpse(rental)
```

### Look at what R sends to `postgreSQL`

NOTE: This may be moved to an earlier chapter, there is no particular reason that it be here:

The equivalent of `rental <- dplyr::tbl(con, "rental")` is:
```{r}
rental %>% dplyr::show_query()
```
## What is in the database?

For large or complex databases, however, you need to use both the available documentation for your database (e.g.,  [the dvdrental](http://www.postgresqltutorial.com/postgresql-sample-database/) database) and the other empirical tools that are available.  For example it's worth learning to interpret the symbols in an [Entity Relationship Diagram](https://en.wikipedia.org/wiki/Entity%E2%80%93relationship_model):

![](./screenshots/ER-diagram-symbols.png)

The `information_schema` is a trove of information *about* the database.  Its format is more or less consistent across the different SQL implementations that are available.   Here we explore some of what's available using several different methods.  Postgres stores [a lot of metadata](https://www.postgresql.org/docs/current/static/infoschema-columns.html).

### Look at what `information_schema` contains
For this chapter R needs the `dbplyr` package to access alternate schemas.  A [schema](http://www.postgresqltutorial.com/postgresql-server-and-database-objects/) is an object that contains one or more tables.  Most often there will be a default schema, but to access the metadata, you need to explicitly specify which schema contains the data you want.

## What tables are in the database?
The simplest way to get a list of tables is with 
```{r}
kable(DBI::dbListTables(con))
```
### Use the `information_schema` to investigate the database

Often we want more detail than just a list of tables.  

The `information_schema` is different from the default, so to connect to the `tables` table we connect to the database in a different way:
```{r}
table_info_schema_table <- tbl(con, dbplyr::in_schema("information_schema", "tables"))
```
The `information_schema` is large and complex and contains `r table_info_schema_table %>% collect %>% dim %>% pluck(1)` tables.

This query retrieves a list of the tables in the database that includes additional detail, not just the name of the table.
```{r}
table_info_schema_table %>%
  filter(table_schema == "public") %>%
  select(table_catalog, table_schema, table_name, table_type) %>%
  arrange(table_type, table_name) %>%
  collect() %>%
  kable()
```
`table_catalog` is synonymous with `database`.

```{r}

table_info_schema_table %>%
  filter(table_schema == "public") %>%  # See alternative below
  select(table_catalog, table_schema, table_name, table_type) %>%
  arrange(table_type, table_name) %>%
  show_query()
```
Notice that VIEWS are composites made up of one or more BASE TABLES.

Since dplyr code is equivalent to SQL, we have a choice.  Also there are different ways of specifying what we want: 

  `WHERE ("table_schema" = 'public')`

is equivalent to:

  `where table_schema not in ('pg_catalog','information_schema')`

The SQL world has its own terminology.  For example `rs` is shorthand for `result set`.  That's equivalent to using `df` for a `data frame`.
```{r}
rs <- dbGetQuery(
  con,
  "select table_catalog, table_schema, table_name, table_type 
  from information_schema.tables 
  where table_schema not in ('pg_catalog','information_schema')
  order by table_type, table_name 
  ;"
)
kable(rs)
```

## What columns do those tables contain?

Of course, the `DBI` package has a `dbListFields` function that provides the simplest way to get the minimum, a list of column names:
```{r}
DBI::dbListFields(con, "rental")
```

But the `information_schema` has a lot more useful information that we can use.  This query retrieves more information about the `rental` table:
```{r}
columns_info_schema_table <- tbl(con, dbplyr::in_schema("information_schema", "columns")) 

columns_info_schema_info <- columns_info_schema_table %>%
  filter(table_schema == "public") %>% 
  select(
    table_catalog, table_schema, table_name, column_name, data_type, ordinal_position,
    character_maximum_length, column_default, numeric_precision, numeric_precision_radix
  ) %>%
  collect(n = Inf) %>% 
  mutate(full_table_name = paste(table_catalog, table_schema, table_name, sep = "."),
         data_type = case_when(
           data_type == "character varying" ~ paste0(data_type, ' (', character_maximum_length, ')'),
           data_type == "real" ~ paste0(data_type, ' (', numeric_precision, ',', numeric_precision_radix,')'),
           TRUE ~ data_type)
         ) %>% 
  filter(table_name == "rental") %>% 
  select(-table_schema, -numeric_precision, -numeric_precision_radix)

glimpse(columns_info_schema_info)

kable(columns_info_schema_info)
```

### What is the difference between a `VIEW` and a `BASE TABLE`?


The `BASE TABLE` has the underlying data in the database
```{r}
table_info_schema_table %>%
  filter(table_schema == "public" & table_type == "BASE TABLE") %>% 
  select(table_name, table_type) %>% 
  left_join(columns_info_schema_table, by = c("table_name" = "table_name")) %>% 
  select(
    table_type, table_name, column_name, data_type, ordinal_position,
    column_default
  ) %>%
  collect(n = Inf) %>% 
  filter(str_detect(table_name, "cust")) %>% 
  kable()
 
```

Probably should explore how the `VIEW` is made up of data from BASE TABLEs.
```{r}
table_info_schema_table %>%
  filter(table_schema == "public" & table_type == "VIEW") %>%  
  select(table_name, table_type) %>% 
  left_join(columns_info_schema_table, by = c("table_name" = "table_name")) %>% 
  select(
    table_type, table_name, column_name, data_type, ordinal_position,
    column_default
  ) %>%
  collect(n = Inf) %>% 
  filter(str_detect(table_name, "cust")) %>% 
  kable()
```

### Counting columns and name reuse
Pull out some rough-and-ready but useful statistics about your database.  Since we are in SQL-land we talk about variables as `columns`.

```{r}
columns_info_schema_table %>%
  filter(table_schema == "public") %>%
  count(table_name, sort = TRUE) %>%
  kable()
```

## Create a list of tables names and a count of the number of columns that each one contains.

How many *column names* are shared across tables (or duplicated)?
```{r}

columns_info_schema_info %>% count(column_name, sort = TRUE) %>% filter(n > 1)
```

How many column names are unique?
```{r}
columns_info_schema_info %>% count(column_name) %>% filter(n == 1) %>% count()
```

What data types are found in the database?
```{r}

columns_info_schema_info %>% count(data_type)
```

### Submitting SQL statements directly

This chapter is about `information_schema` not about direct SQL, so we should only have direct SQL when we know that it's difficult or impossible to construct an equivalent query in dplyr.
```{r}

table_schema_query <- glue(
  "SELECT ",
  "table_name, column_name, data_type, ordinal_position, character_maximum_length, column_default",
  " FROM information_schema.columns ",
  "WHERE table_schema = 'public'"
)

rental_meta_data <- dbGetQuery(con, table_schema_query)
names(rental_meta_data) <- str_replace(names(rental_meta_data), "_", " ")

glimpse(rental_meta_data)
kable(head(rental_meta_data, n = 20))
```


There are `r dim(rs)[1]` rows in the catalog.

What do we learn from the following query?  How is it useful?
```{r}
rs <- dbGetQuery(
  con,
  "
--SELECT conrelid::regclass as table_from
select table_catalog||'.'||table_schema||'.'||table_name table_name
,conname,pg_catalog.pg_get_constraintdef(r.oid, true) as condef
FROM information_schema.columns c,pg_catalog.pg_constraint r
WHERE 1 = 1 --r.conrelid = '16485' 
  AND r.contype  in ('f','p') ORDER BY 1
;"
)
glimpse(rs)
kable(head(rs))
```
What do we learn from the following query?  How is it useful? 
```{r}
rs <- dbGetQuery(
  con,
  "select conrelid::regclass as table_from
      ,c.conname
      ,pg_get_constraintdef(c.oid)
  from pg_constraint c
  join pg_namespace n on n.oid = c.connamespace
 where c.contype in ('f','p')
   and n.nspname = 'public'
order by conrelid::regclass::text, contype DESC;
"
)
glimpse(rs)
kable(head(rs))
dim(rs)[1]
```

This query shows the primary and foreign keys in the database.
```{r}
tables <- tbl(con, dbplyr::in_schema("information_schema", "tables"))
table_constraints <- tbl(con, dbplyr::in_schema("information_schema", "table_constraints"))
key_column_usage <- tbl(con, dbplyr::in_schema("information_schema", "key_column_usage"))
referential_constraints <- tbl(con, dbplyr::in_schema("information_schema", "referential_constraints"))
constraint_column_usage <- tbl(con, dbplyr::in_schema("information_schema", "constraint_column_usage"))

keys <- tables %>% 
  left_join(table_constraints, by = c(
    "table_catalog" = "table_catalog",
    "table_schema" =  "table_schema",
    "table_name" = "table_name"
  )) %>% 
  # table_constraints %>% 
  filter(constraint_type %in% c("FOREIGN KEY", "PRIMARY KEY")) %>% 
  left_join(key_column_usage, 
            by = c(
              "table_catalog" = "table_catalog",
              "constraint_catalog" = "constraint_catalog",
              "constraint_schema" = "constraint_schema",
              "table_name" = "table_name",
              "table_schema" = "table_schema",
              "constraint_name" = "constraint_name"
              )) %>%
  # left_join(constraint_column_usage) %>% # does this table add anything useful?
  select(table_name, table_type, constraint_name, constraint_type, column_name, ordinal_position) %>%
  arrange(table_name) %>% 
collect()
glimpse(keys)
kable(keys)
```

What do we learn from the following query?  How is it useful? 
```{r}
rs <- dbGetQuery(
  con,
  "SELECT r.*,
  pg_catalog.pg_get_constraintdef(r.oid, true) as condef
FROM pg_catalog.pg_constraint r
WHERE 1=1 --r.conrelid = '16485' AND r.contype = 'f' ORDER BY 1;
"
)

head(rs)
```
