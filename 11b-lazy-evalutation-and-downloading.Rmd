# Lazy Evaluation and Lazy Queries (11b)


## This chapter:
> 
> * Reviews lazy evaluation and discusses its interaction with remote query execution on a dbms 
> * Illustrates some of the differences between writing `dplyr` commands and SQL
> * Suggests some strategies for dividing the work between your local R session and the dbms

### Setup

The following packages are used in this chapter:
```{r chapter package list, echo=TRUE, message=FALSE, warning=FALSE}
library(tidyverse)
library(DBI)
library(RPostgres)
library(dbplyr)
require(knitr)
library(bookdown)
library(sqlpetr)
```
Assume that the Docker container with PostgreSQL and the dvdrental database are ready to go. If not go back to [the previous Chapter][Build the pet-sql Docker Image]
```{r check on sql-pet}
sp_docker_start("sql-pet")
```
Connect to the database:
```{r connect to postgresql}
con <- sp_get_postgres_connection(
  user = Sys.getenv("DEFAULT_POSTGRES_USER_NAME"),
  password = Sys.getenv("DEFAULT_POSTGRES_PASSWORD"),
  dbname = "dvdrental",
  seconds_to_test = 10
)
```

## R is lazy and comes with guardrails

By design, R is both a language and an interactive development environment (IDE).  As a language, R tries to be as efficient as possible.  As an IDE, R creates some guardrails to make it easy and safe to work with your data. For example `getOption("max.print")` prevents R from printing more rows of data than you can handle, with a nice default of `r getOption("max.print")`, which may or may not suit you.

On the other hand SQL a *"Structured Query Language (SQL) is a standard computer language for relational database management and data manipulation."* ^[https://www.techopedia.com/definition/1245/structured-query-language-sql]. SQL has database-specific Interactive Development Environments (IDEs): for postgreSQL it's [pgAdmin](https://www.pgadmin.org/).  Roger Peng explains in [R Programming for Data Science](https://bookdown.org/rdpeng/rprogdatascience/history-and-overview-of-r.html#basic-features-of-r) that:

> R has maintained the original S philosophy, which is that it provides a language that is both useful for interactive work, but contains a powerful programming language for developing new tools. 

This is complicated when R interacts with SQL.  In [the vignette for dbplyr](https://cran.r-project.org/web/packages/dbplyr/vignettes/dbplyr.html) Hadley Wikham explains:

> The most important difference between ordinary data frames and remote database queries is that your R code is translated into SQL and executed in the database on the remote server, not in R on your local machine. When working with databases, dplyr tries to be as lazy as possible:
> 
> * It never pulls data into R unless you explicitly ask for it.
> 
> * It delays doing any work until the last possible moment: it collects together everything you want to do and then sends it to > the database in one step.
> 

Eventually, if you are interacting with a dbms from R you will need to understand the differences between lazy loading, lazy evaluation, and lazy queries.

### Lazy loading

"*Lazy loading is always used for code in packages but is optional (selected by the package maintainer) for datasets in packages.*"^[https://cran.r-project.org/doc/manuals/r-release/R-ints.html#Lazy-loading]  Lazy loading means that the code for a particular function doesn't actually get loaded into memory until the last minute -- when it's actually being used.

### Lazy evaluation 

Essentially "Lazy evaluation is a programming strategy that allows a symbol to be evaluated only when needed." ^[https://colinfay.me/lazyeval/]  That means that lazy evaluation is about **symbols** such as function arguments ^[http://adv-r.had.co.nz/Functions.html#function-arguments] when they are evaluated. Tidy evaluation complicates lazy evaluation. ^[https://colinfay.me/tidyeval-1/]

### Lazy Queries

"*When you create a "lazy" query, you're creating a pointer to a set of conditions on the database, but the query isn't actually run and the data isn't actually loaded until you call "next" or some similar method to actually fetch the data and load it into an object.*" ^[https://www.quora.com/What-is-a-lazy-query]  The `collect()` function retrieves data into a local tibble.^[https://dplyr.tidyverse.org/reference/compute.html]

## Lazy evaluation and lazy queries

### Dplyr connection objects
As introduced in the previous chapter, the `dplyr::tbl` function creates  an object that might **look** like a data frame in that when you enter it on the command line, it prints a bunch of rows from the dbms table.  But is actually a **list** object that `dplyr` uses for constructing queries and retrieving data from the DBMS.  

The following code illustrates these issues.  The `dplyr::tbl` function creates the connection object that we store in an object named `rental_table`:
```{r}
rental_table <- dplyr::tbl(con, "rental")
```
At first glance, it kind of **looks** like a data frame although it only prints 10 of the table's 16,044 rows:
```{r}
rental_table
```
But consider the structure of  `rental_table`:
```{r}
str(rental_table)
```

It has two rows.  The first row contains all the information in the `con` object, which contains information about all the tables and objects in the database:
```{r}
rental_table$src$con@typnames$typname[380:437]
```
The second row contains a list of the columns in the `rental` table, among other things:

```{r}
rental_table$ops$vars
```

To illustrate the different issues involved in data retrieval, we create equivalent connection objects to link to two other tables.  
```{r}
staff_table <- dplyr::tbl(con, "staff") 
# the 'staff' table has 2 rows

customer_table <- dplyr::tbl(con, "customer") 
# the 'customer' table has 599 rows
```

### Using a lazy query

Here is a typical string of dplyr verbs strung together with the magrittr `%>%` command that will be used to tease out the several different behaviors that a lazy query has when passed to different R functions.  This query joins three connection objects into a query we'll call `Q`:

```{r}
Q <- rental_table %>%
  select(staff_id, customer_id, rental_date) %>%
  left_join(staff_table, by = c("staff_id" = "staff_id")) %>%
  rename(staff_email = email) %>%
  select(staff_id, customer_id, rental_date, staff_email) %>%
  left_join(customer_table, by = c("customer_id" = "customer_id")) %>%
  rename(customer_email = email) %>%
  select(rental_date, staff_email, customer_email)
```

Think of `Q` as a black box for the moment.  The following examples will show how `Q` is interpreted differently by different functions. 

> R code | Result 
> -------| --------------
> `Q %>% print()` | Prints x rows; same as just entering `Q`  
> `Q %>% as.tibble()` | Forces `Q` to be a tibble
> `Q %>% head()` |  Prints the first 6 rows 
> `Q %>% length()` |  Counts the rows in `Q`
> `Q %>% str(max.level = 3)` | Shows the top 3 levels of the **object** `Q` 
> `Q %>% nrow()` | **Attempts** to determine the number of rows 
> `Q %>% tally()` | Counts all the rows -- on the dbms side
> `Q %>% collect (n = 20)` | Prints 20 rows  
> `Q %>% collect (n = 20) %>% head()` | Prints 6 rows  
> `Q %>% show_query()` | **Translates** the lazy query object into SQL  
> `Qc <- Q %>%` <br /> `count(customer_email, sort = TRUE)` <br /> `Qc` | **Extends** the lazy query object
>
> 

(The next chapter will discuss how to build queries and how to explore intermediate steps.)

Remember that `Q %>% print()` is equivalent to `print(Q)` and the same as just entering `Q` on the command line.  We use the magrittr pipe operator here because chaining functions highlights how the same object behaves differently in each use.
```{r}
Q %>% print()
```
In its role as IDE, R has provided nicely formatted output that is similar to what it prints for a tibble, with descriptive information about the dataset and each column:

>
> # Source:   lazy query [?? x 3]
> # Database: postgres [postgres@localhost:5432/dvdrental]
>   rental_date         staff_email                  customer_email                       
<   <dttm>              <chr>                        <chr>
>

It has only retrieved 10 rows but doesn't know how many rows are left to retrieve as it notes `... with more rows`. 
In contrast to `print()`, the `as.tibble()` function causes R to download the whole table, using tibble's default of displaying only the first 10 rows.
```{r}
Q %>% as.tibble()
```

The `head()` function is very similar to print but has a different "`max.print`" value.
```{r}
Q %>% head()
```
Because the `Q` object is relatively complex, using `str()` on it prints many lines.  You can glimpse what's going on with `length()`:
```{r}
Q %>% length()
```
Looking inside shows some of what's going on:
```{r}
Q %>% str(max.level = 3) 
```
Notice the difference between `nrow()` and `tally()`:
```{r}
Q %>% nrow()
Q %>% tally()
```
The `nrow()` function knows that `Q` is a list.  On the other hand, the `tally()` function tells SQL to go count all the rows. Notice that `Q` results in 16,044 rows -- the same number of rows as `rental`.

The `dplyr::collect()` function triggers a dbFetch() function behind the scenes, which forces R to download a specified number of rows:
```{r}
Q %>% collect(n = 20)
Q %>% collect(n = 20) %>% head()
```
The `collect` function triggers the creation of a tibble and controls the number of rows that the DBMS sends to R.  Notice that `head` only prints 6 of the 25 rows that R has retrieved.  

```{r}
Q %>% show_query()

```
Hand-written SQL code to do the same job will probably look a lot nicer and could be more efficient, but functionally dplyr does the job.

But because `Q` hasn't been executed, we can add to it.  This behavior is the basis for a useful debugging and development process where queries are built up incrementally.
```{r}
Qc <- Q %>% count(customer_email, sort = TRUE) 
Qc
```

See more example of lazy execution can be found [Here](https://datacarpentry.org/R-ecology-lesson/05-r-and-databases.html).

## SQL in R Markdown

When you create a report to run repeatedly, you might want to put that query into R markdown. See the discussion of [multiple language engines in R Markdown](https://bookdown.org/yihui/rmarkdown/language-engines.html#sql). That way you can also execute that SQL code in a chunk with the following header:

  {`sql, connection=con, output.var = "query_results"`}

```{sql, connection=con, output.var = "query_results"}
SELECT "staff_id", COUNT(*) AS "n"
FROM "rental"
GROUP BY "staff_id";
```
Rmarkdown stored that query result in a tibble:
```{r}
query_results

```
## DBI Package

In this chapter we touched on a number of functions from the DBI Package.  The table in file 96b shows other functions in the package.  The Chapter column references a section in the book if we have used it.

```{r}
film_table <- tbl(con, "film")
```

### Retrieve the whole table

SQL code that is submitted to a database is evaluated all at once^[From R's perspective. Actually there are 4 steps behind the scenes.].  To think through an SQL query, either use dplyr to build it up step by step and then convert it to SQL code or an IDE such as [pgAdmin](https://www.pgadmin.org/). DBI returns a data.frame, so you don't have dplyr's guardrails.
```{r}
res <- dbSendQuery(con, 'SELECT "title", "rental_duration", "length"
FROM "film"
WHERE ("rental_duration" > 5.0 AND "length" > 117.0)')

res_output <- dbFetch(res)
str(res_output)

dbClearResult(res)
```

### Or a chunk at a time

```{r}
res <- dbSendQuery(con, 'SELECT "title", "rental_duration", "length"
FROM "film"
WHERE ("rental_duration" > 5.0 AND "length" > 117.0)')

chunk_num <- 0
while(!dbHasCompleted(res)){
  chunk_num <- chunk_num + 1
  chunk <- dbFetch(res, n = 5)
  print(nrow(chunk))
  if (!chunk_num %% 7) {print(chunk)}
}

dbClearResult(res)
```

## Dividing the work between R on your machine and the DBMS

They work together.

### Make the server do as much work as you can

* show_query as a first draft of SQL.  May or may not use SQL code submitted directly.

### Criteria for choosing between `dplyr` and native SQL

This probably belongs later in the book.

* performance considerations: first get the right data, then worry about performance
* Trade offs between leaving the data in PostgreSQL vs what's kept in R: 
  + browsing the data
  + larger samples and complete tables
  + using what you know to write efficient queries that do most of the work on the server

Where you place the `collect` function matters.
```{r}
dbDisconnect(con)
sp_docker_stop("sql-pet")
```


## Other resources

  * Benjamin S. Baumer, A Grammar for Reproducible and Painless Extract-Transform-Load Operations on Medium Data: [https://arxiv.org/pdf/1708.07073](https://arxiv.org/pdf/1708.07073) 

